{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthShah412/HelloGit/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GiSe73r8KHis",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from tests import test_prediction, test_generation\n",
        "from torch.optim import Adam\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = ('cpu')\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ICv2Unw3KHjb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bb7cd179-5e9d-4be3-870c-570e7021b2df"
      },
      "cell_type": "code",
      "source": [
        "# load all that we need\n",
        "print(device)\n",
        "dataset = np.load('/content/handout/dataset/wiki.train.npy')\n",
        "fixtures_pred = np.load('/content/handout/fixtures/prediction.npz')  # dev\n",
        "fixtures_gen = np.load('/content/handout/fixtures/generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('/content/handout/fixtures/prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('/content/handout/fixtures/generation_test.npy')  # test\n",
        "vocab = np.load('/content/handout/dataset/vocab.npy')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QXwhB78bKHj6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "189b18d8-3be4-4ed6-8019-9fa0409ddd46"
      },
      "cell_type": "code",
      "source": [
        "print(fixtures_pred['inp'][1])\n",
        "print(fixtures_pred['out'][1])\n",
        "strn = ' '.join(vocab[word] for word in fixtures_pred['inp'][1])\n",
        "print(strn, vocab[fixtures_pred['out'][1]], vocab[24820])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14658 21076 21626 31353  6613  1419 10706 15340 25874 25949 31994 21626\n",
            "  2299  3952    79  1419    76  1184 31543  1242]\n",
            "24820\n",
            "a few from the Heavy <unk> Platoon and one or two from B Company . <unk> , 60 to 70 men men\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z8BLcKidKHlK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data loader\n",
        "\n",
        "class LanguageModelDataLoader(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        # concatenate your articles and build into batches\n",
        "        \n",
        "        np.random.shuffle(self.dataset)\n",
        "        shuffled_data = self.dataset\n",
        "        concatenated_data = []\n",
        "        for i in shuffled_data:\n",
        "            concatenated_data.extend(i)\n",
        "        concatenated_data = np.asarray(concatenated_data)\n",
        "        new_len = concatenated_data.shape[0] - concatenated_data.shape[0]%self.batch_size\n",
        "        temp_data = concatenated_data[:new_len]\n",
        "        concatenated_data = temp_data.reshape(self.batch_size, temp_data.shape[0]//self.batch_size)\n",
        "        seq_len = 0\n",
        "        j = 0\n",
        "        while(j<concatenated_data.shape[1]):\n",
        "            tmp = np.random.randn(1)\n",
        "            if(tmp > 0.05):\n",
        "                seq_len = np.random.normal(70,5,1)\n",
        "            else:\n",
        "                seq_len = np.random.normal(35,5,1)\n",
        "            if(j+seq_len<(concatenated_data.shape[1])):\n",
        "                batch_arr = np.zeros((self.batch_size,int(seq_len)-1))\n",
        "                batch_otp = np.zeros((self.batch_size,int(seq_len)-1))\n",
        "                for i in range(concatenated_data.shape[0]-1):\n",
        "                    batch_arr[i] = concatenated_data[i,j:j+int(seq_len)-1]\n",
        "                    batch_otp[i] = concatenated_data[i,j+1:j+int(seq_len)]\n",
        "                j = j+int(seq_len)\n",
        "                yield (batch_arr, batch_otp)\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdVDScarKHli",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "load_data = LanguageModelDataLoader(dataset,80)\n",
        "k = 0\n",
        "for i,(j,k) in enumerate(load_data):\n",
        "    print(j.shape, k.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NJsT2cnYKHl3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "        TODO: Define your model here\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, nlayers):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.nlayers=nlayers\n",
        "        self.embedding = nn.Embedding(vocab_size,embed_size) # Embedding layer\n",
        "        self.rnn = nn.LSTM(input_size = embed_size,hidden_size=hidden_size,num_layers=nlayers, bidirectional=True) # Recurrent network\n",
        "        self.scoring = nn.Linear(2*hidden_size,vocab_size) # Projection layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(1)\n",
        "        embeddings = self.embedding(x)\n",
        "        output_lstm,hidden = self.rnn(embeddings)\n",
        "        embeddings = embeddings.detach().cpu().numpy()\n",
        "        del embeddings\n",
        "        output_lstm_flatten = output_lstm.view(-1,2*self.hidden_size)\n",
        "        output_flatten = self.scoring(output_lstm_flatten)\n",
        "        output_lstm_flatten = output_lstm_flatten.detach().cpu().numpy()\n",
        "        del output_lstm_flatten\n",
        "        torch.cuda.empty_cache()\n",
        "        return output_flatten.view(-1, batch_size, self.vocab_size)\n",
        "        \n",
        "        \n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z-hoWFNUKHmP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=len(vocab), embed_size=400, hidden_size = 1150, nlayers=3)\n",
        "model.to(device)\n",
        "for i,(j,k) in enumerate(load_data):\n",
        "    inputs = torch.LongTensor(j).to(device)\n",
        "    output = model(inputs)\n",
        "    print(output.shape)\n",
        "#     output = output.detach().cpu().numpy()\n",
        "    inputs = inputs.detach().cpu().numpy()\n",
        "    del output\n",
        "    del inputs\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b7ul2OALKHms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model trainer\n",
        "\n",
        "class LanguageModelTrainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        self.optimizer = Adam(model.parameters(), lr = 0.001)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "            print(epoch_loss)\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        inputs = torch.LongTensor(inputs).to(device)\n",
        "        inputs = torch.transpose(inputs,0,1).to(device)\n",
        "        targets = torch.LongTensor(targets).to(device)\n",
        "        targets = torch.transpose(targets,0,1).to(device)\n",
        "        outputs = model(inputs)\n",
        "        targets = targets.contiguous()\n",
        "        outputs = outputs.view(-1,outputs.size(2))\n",
        "        loss = self.criterion(outputs, targets.view(-1))\n",
        "        #loss = Variable(loss, requires_grad=True)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "        \n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8RpODQW7KHnH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TestLanguageModel:\n",
        "    def prediction(inp, model):\n",
        "        with torch.no_grad():\n",
        "            outputs_array = np.zeros((inp.shape[0],len(vocab)))\n",
        "            for i in range(inp.shape[0]):\n",
        "                inputs = torch.LongTensor(inp[0]).to(device)\n",
        "                inputs = torch.transpose(inputs,0,1)\n",
        "                outptuts = model(inputs)\n",
        "                outputs_array[i] = outputs[-1,:,:].numpy()\n",
        "        return outputs_array\n",
        "            \n",
        "    def generation(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"        \n",
        "        with torch.no_grad():\n",
        "            outputs_array = np.zeros((inp.shape[0],forward))\n",
        "            for i in range(inp.shape[0]):\n",
        "                inputs = torch.LongTensor(inp[0]).to(device)\n",
        "                inputs = torch.transpose(inputs,0,1)\n",
        "                outptuts = model(inputs)\n",
        "                temp = outputs[-1,:,:]\n",
        "                outputs_array[i,0] = torch.max(temp,dim=1).numpy()\n",
        "                for j in range(1,forward):\n",
        "                    word = outputs_array[i,j-1]\n",
        "                    word = torch.LongTensor(word).view(1,1).to(device)\n",
        "                    output = model(word)\n",
        "                    temp = outputs[0,0,:]\n",
        "                    outputs_array[i,j] = torch.max(temp,dim=1).numpy()\n",
        "        return outputs_array\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1VPlk-A7KHnv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "NUM_EPOCHS = 8\n",
        "BATCH_SIZE = 80\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jBoB_JYoKHn7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2e20b98b-1acc-44bc-9f5c-8e0e08de9361"
      },
      "cell_type": "code",
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1555368232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OjLlJCs_KHog",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=len(vocab), embed_size=400, hidden_size = 1150, nlayers=3)\n",
        "model.to(device)\n",
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cMh7CDfGKHo1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 767
        },
        "outputId": "19c7a2f9-c7f2-4947-fdc2-d49d09d24d6e"
      },
      "cell_type": "code",
      "source": [
        "best_nll = 1e30 \n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d424a62b4676>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnll\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_nll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-ad57fbf6c345>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-ad57fbf6c345>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-6fbf1623a1e5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moutput_lstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moutput_lstm_flatten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "lEeaM4c4KHp4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y24qSj20KHqE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ASYz14gKHqW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}