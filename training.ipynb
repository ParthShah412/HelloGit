{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthShah412/HelloGit/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GiSe73r8KHis",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tests import test_prediction, test_generation\n",
        "from torch.optim import Adam\n",
        "import pdb\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = ('cpu')\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ICv2Unw3KHjb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "155472a8-ab9c-46ef-87da-50aa1b0897ca"
      },
      "cell_type": "code",
      "source": [
        "# load all that we need\n",
        "print(device)\n",
        "dataset = np.load('/content/handout/dataset/wiki.train.npy')\n",
        "fixtures_pred = np.load('/content/handout/fixtures/prediction.npz')  # dev\n",
        "fixtures_gen = np.load('/content/handout/fixtures/generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('/content/handout/fixtures/prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('/content/handout/fixtures/generation_test.npy')  # test\n",
        "vocab = np.load('/content/handout/dataset/vocab.npy')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QXwhB78bKHj6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "83720ef1-4716-4836-91b2-c8c9e1a00bb2"
      },
      "cell_type": "code",
      "source": [
        "print(fixtures_pred['inp'][1])\n",
        "print(fixtures_pred['out'][1])\n",
        "strn = ' '.join(vocab[word] for word in fixtures_pred['inp'][1])\n",
        "print(strn, vocab[fixtures_pred['out'][1]], vocab[24820])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14658 21076 21626 31353  6613  1419 10706 15340 25874 25949 31994 21626\n",
            "  2299  3952    79  1419    76  1184 31543  1242]\n",
            "24820\n",
            "a few from the Heavy <unk> Platoon and one or two from B Company . <unk> , 60 to 70 men men\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z8BLcKidKHlK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data loader\n",
        "\n",
        "class LanguageModelDataLoader(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        # concatenate your articles and build into batches\n",
        "        \n",
        "        np.random.shuffle(self.dataset)\n",
        "        shuffled_data = self.dataset\n",
        "        concatenated_data = []\n",
        "        for i in shuffled_data:\n",
        "            concatenated_data.extend(i)\n",
        "        concatenated_data = np.asarray(concatenated_data)\n",
        "        new_len = concatenated_data.shape[0] - concatenated_data.shape[0]%self.batch_size\n",
        "        temp_data = concatenated_data[:new_len]\n",
        "        concatenated_data = temp_data.reshape(self.batch_size, temp_data.shape[0]//self.batch_size)\n",
        "        seq_len = 0\n",
        "        j = 0\n",
        "        while(j<concatenated_data.shape[1]):\n",
        "            tmp = np.random.randn(1)\n",
        "            if(tmp > 0.05):\n",
        "                seq_len = np.random.normal(70,5,1)\n",
        "            else:\n",
        "                seq_len = np.random.normal(35,5,1)\n",
        "            if(j+seq_len<(concatenated_data.shape[1])):\n",
        "                batch_arr = np.zeros((self.batch_size,int(seq_len)-1))\n",
        "                batch_otp = np.zeros((self.batch_size,int(seq_len)-1))\n",
        "                for i in range(concatenated_data.shape[0]-1):\n",
        "                    batch_arr[i] = concatenated_data[i,j:j+int(seq_len)-1]\n",
        "                    batch_otp[i] = concatenated_data[i,j+1:j+int(seq_len)]\n",
        "                j = j+int(seq_len)\n",
        "                yield (batch_arr, batch_otp)\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdVDScarKHli",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "load_data = LanguageModelDataLoader(dataset,80)\n",
        "k = 0\n",
        "for i,(j,k) in enumerate(load_data):\n",
        "    print(j.shape, k.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NJsT2cnYKHl3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "        TODO: Define your model here\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, nlayers):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.nlayers=nlayers\n",
        "        self.embedding = nn.Embedding(vocab_size,embed_size) # Embedding layer\n",
        "        self.rnn = nn.LSTM(input_size = embed_size,hidden_size=hidden_size,num_layers=nlayers, bidirectional=True) # Recurrent network\n",
        "        self.scoring = nn.Linear(2*hidden_size,vocab_size) # Projection layer\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        batch_size = x.size(1)\n",
        "        embeddings = self.embedding(x)\n",
        "        output_lstm,hidden = self.rnn(embeddings, hidden)\n",
        "        embeddings = embeddings.detach().cpu().numpy()\n",
        "        del embeddings\n",
        "        output_lstm_flatten = output_lstm.view(-1,2*self.hidden_size)\n",
        "        output_flatten = self.scoring(output_lstm_flatten)\n",
        "        output_lstm_flatten = output_lstm_flatten.detach().cpu().numpy()\n",
        "        del output_lstm_flatten\n",
        "        torch.cuda.empty_cache()\n",
        "        return output_flatten.view(-1, batch_size, self.vocab_size), hidden\n",
        "        \n",
        "        \n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z-hoWFNUKHmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "c67483ff-4f9c-4e4a-fb30-b534e8ef8cf8"
      },
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=len(vocab), embed_size=400, hidden_size = 1150, nlayers=3)\n",
        "model.to(device)\n",
        "for i,(j,k) in enumerate(load_data):\n",
        "    inputs = torch.LongTensor(j).to(device)\n",
        "    output = model(inputs)\n",
        "    print(output.shape)\n",
        "#     output = output.detach().cpu().numpy()\n",
        "    inputs = inputs.detach().cpu().numpy()\n",
        "    del output\n",
        "    del inputs\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-144ed795ce99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "b7ul2OALKHms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model trainer\n",
        "\n",
        "class LanguageModelTrainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        self.optimizer = Adam(model.parameters(), lr = 0.001)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "            print(epoch_loss)\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        inputs = torch.LongTensor(inputs).to(device)\n",
        "        inputs = torch.transpose(inputs,0,1).to(device)\n",
        "        targets = torch.LongTensor(targets).to(device)\n",
        "        targets = torch.transpose(targets,0,1).to(device)\n",
        "        hidden = None\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        targets = targets.contiguous()\n",
        "        outputs = outputs.view(-1,outputs.size(2))\n",
        "        loss = self.criterion(outputs, targets.view(-1))\n",
        "        #loss = Variable(loss, requires_grad=True)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "        \n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8RpODQW7KHnH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TestLanguageModel:\n",
        "    def prediction(inp, model):\n",
        "        with torch.no_grad():\n",
        "            outputs_array = np.zeros((inp.shape[0],len(vocab)))\n",
        "            inputs = torch.LongTensor(inp).to(device)\n",
        "            inputs = torch.transpose(inputs,0,1)\n",
        "            hidden=None\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            outputs_array = outputs[-1,:,:].cpu().numpy()\n",
        "            outputs_array = outputs_array.astype(np.float64)\n",
        "        return outputs_array\n",
        "            \n",
        "    def generation(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"\n",
        "        generated_output = []\n",
        "        with torch.no_grad():\n",
        "            inputs = torch.LongTensor(inp).to(device)\n",
        "            inputs = torch.transpose(inputs,0,1)\n",
        "            hidden = None\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            temp = outputs[-1,:,:]\n",
        "            _,current_word = torch.max(temp,dim=1)\n",
        "            generated_output.append(current_word.cpu().numpy())\n",
        "            print(\"Current_word\",current_word)\n",
        "            for j in range(1,forward):\n",
        "              word = current_word\n",
        "              print(word.shape)\n",
        "              word = word.view(1,-1)\n",
        "              print(word.shape)\n",
        "              output, hidden = model(word, hidden)\n",
        "              _,current_word = torch.max(temp,dim=1)\n",
        "              generated_output.append(current_word.cpu().numpy())\n",
        "        generated_matrix = np.asarray(generated_output)\n",
        "        print(generated_matrix.shape)\n",
        "        return torch.Tensor(generated_matrix, requires_grad=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1VPlk-A7KHnv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "NUM_EPOCHS = 8\n",
        "BATCH_SIZE = 80\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jBoB_JYoKHn7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7da69dae-b197-46ba-ea02-377ada4760d7"
      },
      "cell_type": "code",
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1555378092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OjLlJCs_KHog",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=len(vocab), embed_size=400, hidden_size = 1150, nlayers=3)\n",
        "model.to(device)\n",
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B5gMwgBzbMa6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1047
        },
        "outputId": "74ecbc64-7b7f-4b26-c348-ac82ea76b51b"
      },
      "cell_type": "code",
      "source": [
        "best_nll = 1e30\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  nll = trainer.test()\n",
        "  if nll < best_nll:\n",
        "    best_nll = nll\n",
        "    print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current_word tensor([21051, 18696, 16978,  2115, 14764, 14764,  7469,  5814, 30200, 14764,\n",
            "        17401, 21051, 26632, 14764, 11365, 15341, 21051, 22180, 25375, 19290,\n",
            "         9244,   386, 20122, 21051,  7469,  9244,  2115, 18815, 21051, 21051,\n",
            "         4869,  6900], device='cuda:0')\n",
            "torch.Size([32])\n",
            "torch.Size([1, 32])\n",
            "torch.Size([32])\n",
            "torch.Size([1, 32])\n",
            "torch.Size([32])\n",
            "torch.Size([1, 32])\n",
            "torch.Size([32])\n",
            "torch.Size([1, 32])\n",
            "torch.Size([32])\n",
            "torch.Size([1, 32])\n",
            "torch.Size([32])\n",
            "torch.Size([1, 32])\n",
            "torch.Size([32])\n",
            "torch.Size([1, 32])\n",
            "torch.Size([32])\n",
            "torch.Size([1, 32])\n",
            "torch.Size([32])\n",
            "torch.Size([1, 32])\n",
            "(10, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-cd40c0a92409>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnll\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_nll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbest_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-71b254ad20f3>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestLanguageModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixtures_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inp'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mgenerated_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestLanguageModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixtures_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# generated predictions for 10 words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mgenerated_logits_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTestLanguageModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixtures_gen_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixtures_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-bb4393406506>\u001b[0m in \u001b[0;36mgeneration\u001b[0;34m(inp, forward, model)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mgenerated_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (numpy.ndarray, requires_grad=bool), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the keywords were incorrect: requires_grad\n * (object data, torch.device device)\n      didn't match because some of the keywords were incorrect: requires_grad\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "cMh7CDfGKHo1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4296
        },
        "outputId": "f250a2d6-8fd6-45a6-b3b5-d5428a92ae78"
      },
      "cell_type": "code",
      "source": [
        "best_nll = 1e30 \n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "    "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2640, 33278])\n",
            "tensor(10.4096, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5280, 33278])\n",
            "tensor(20.6466, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5760, 33278])\n",
            "tensor(29.2660, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([3040, 33278])\n",
            "tensor(39.2463, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5600, 33278])\n",
            "tensor(47.3144, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2480, 33278])\n",
            "tensor(55.1354, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2560, 33278])\n",
            "tensor(62.7798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5920, 33278])\n",
            "tensor(70.6764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2000, 33278])\n",
            "tensor(78.5453, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([3200, 33278])\n",
            "tensor(86.3893, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2640, 33278])\n",
            "tensor(94.3077, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([4960, 33278])\n",
            "tensor(102.1863, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5760, 33278])\n",
            "tensor(110.1063, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2480, 33278])\n",
            "tensor(117.8571, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2640, 33278])\n",
            "tensor(125.7411, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5600, 33278])\n",
            "tensor(133.4385, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2320, 33278])\n",
            "tensor(141.0211, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2240, 33278])\n",
            "tensor(148.4104, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5520, 33278])\n",
            "tensor(155.7378, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([6160, 33278])\n",
            "tensor(162.9330, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([3120, 33278])\n",
            "tensor(170.1391, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2160, 33278])\n",
            "tensor(177.1756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2720, 33278])\n",
            "tensor(184.1167, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([3040, 33278])\n",
            "tensor(191.0334, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([1600, 33278])\n",
            "tensor(197.9129, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2400, 33278])\n",
            "tensor(204.6826, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2160, 33278])\n",
            "tensor(211.3873, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2480, 33278])\n",
            "tensor(218.0036, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2560, 33278])\n",
            "tensor(224.5947, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5760, 33278])\n",
            "tensor(231.2213, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([3600, 33278])\n",
            "tensor(237.8152, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5520, 33278])\n",
            "tensor(244.3869, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([4880, 33278])\n",
            "tensor(250.9086, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2800, 33278])\n",
            "tensor(257.2787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([6400, 33278])\n",
            "tensor(263.6724, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2960, 33278])\n",
            "tensor(270.0153, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2880, 33278])\n",
            "tensor(276.2471, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5760, 33278])\n",
            "tensor(282.4273, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5600, 33278])\n",
            "tensor(288.5337, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2480, 33278])\n",
            "tensor(294.6213, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([4640, 33278])\n",
            "tensor(300.7097, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5120, 33278])\n",
            "tensor(306.5149, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2960, 33278])\n",
            "tensor(312.5193, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5200, 33278])\n",
            "tensor(318.4528, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2560, 33278])\n",
            "tensor(324.4044, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2800, 33278])\n",
            "tensor(330.2122, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([4960, 33278])\n",
            "tensor(335.8948, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5680, 33278])\n",
            "tensor(341.5513, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2560, 33278])\n",
            "tensor(347.2463, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2320, 33278])\n",
            "tensor(352.8871, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2000, 33278])\n",
            "tensor(358.4041, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5200, 33278])\n",
            "tensor(363.9966, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5600, 33278])\n",
            "tensor(369.4775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5440, 33278])\n",
            "tensor(374.9302, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5600, 33278])\n",
            "tensor(380.2618, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([4880, 33278])\n",
            "tensor(385.7064, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2480, 33278])\n",
            "tensor(391.1254, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5440, 33278])\n",
            "tensor(396.4958, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5120, 33278])\n",
            "tensor(401.8545, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2800, 33278])\n",
            "tensor(407.0565, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2160, 33278])\n",
            "tensor(412.4524, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2720, 33278])\n",
            "tensor(417.7590, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5040, 33278])\n",
            "tensor(422.9185, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5680, 33278])\n",
            "tensor(428.0563, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5360, 33278])\n",
            "tensor(433.1552, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([3280, 33278])\n",
            "tensor(438.1335, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5520, 33278])\n",
            "tensor(443.2783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([6480, 33278])\n",
            "tensor(448.3037, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5360, 33278])\n",
            "tensor(453.3303, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([4880, 33278])\n",
            "tensor(458.3130, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2320, 33278])\n",
            "tensor(463.4500, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2480, 33278])\n",
            "tensor(468.4068, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2160, 33278])\n",
            "tensor(473.4351, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5360, 33278])\n",
            "tensor(478.2727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2560, 33278])\n",
            "tensor(483.1292, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2640, 33278])\n",
            "tensor(487.8883, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5360, 33278])\n",
            "tensor(492.6288, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2560, 33278])\n",
            "tensor(497.5284, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([4960, 33278])\n",
            "tensor(502.2891, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5040, 33278])\n",
            "tensor(506.9684, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([3440, 33278])\n",
            "tensor(511.7839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([6000, 33278])\n",
            "tensor(516.4655, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5440, 33278])\n",
            "tensor(521.0937, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5120, 33278])\n",
            "tensor(525.6503, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5440, 33278])\n",
            "tensor(530.1860, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2960, 33278])\n",
            "tensor(534.7947, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([3200, 33278])\n",
            "tensor(539.3769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5440, 33278])\n",
            "tensor(543.8705, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([4560, 33278])\n",
            "tensor(548.3483, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5760, 33278])\n",
            "tensor(552.7264, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5520, 33278])\n",
            "tensor(557.2422, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([4960, 33278])\n",
            "tensor(561.5261, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2960, 33278])\n",
            "tensor(565.8550, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([3120, 33278])\n",
            "tensor(570.2820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([5440, 33278])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d424a62b4676>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_nll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnll\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_nll\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1f2eebb9c927>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-1f2eebb9c927>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#loss = Variable(loss, requires_grad=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "lEeaM4c4KHp4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y24qSj20KHqE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-ASYz14gKHqW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}