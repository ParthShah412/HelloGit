{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthShah412/HelloGit/blob/master/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GiSe73r8KHis",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tests import test_prediction, test_generation\n",
        "from torch.optim import Adam\n",
        "import pdb\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = ('cpu')\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ICv2Unw3KHjb",
        "colab_type": "code",
        "outputId": "886790b9-4020-49ea-875a-21ed3ff02243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "# load all that we need\n",
        "print(device)\n",
        "dataset = np.load('/content/handout/dataset/wiki.train.npy')\n",
        "fixtures_pred = np.load('/content/handout/fixtures/prediction.npz')  # dev\n",
        "fixtures_gen = np.load('/content/handout/fixtures/generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('/content/handout/fixtures/prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('/content/handout/fixtures/generation_test.npy')  # test\n",
        "vocab = np.load('/content/handout/dataset/vocab.npy')"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QXwhB78bKHj6",
        "colab_type": "code",
        "outputId": "9fe4b738-157f-4319-8d87-af75312b1928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "cell_type": "code",
      "source": [
        "print(fixtures_pred['inp'][1])\n",
        "print(fixtures_pred['out'][1])\n",
        "strn = ' '.join(vocab[word] for word in fixtures_pred['inp'][1])\n",
        "print(strn, vocab[fixtures_pred['out'][1]], vocab[24820])"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14658 21076 21626 31353  6613  1419 10706 15340 25874 25949 31994 21626\n",
            "  2299  3952    79  1419    76  1184 31543  1242]\n",
            "24820\n",
            "a few from the Heavy <unk> Platoon and one or two from B Company . <unk> , 60 to 70 men men\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z8BLcKidKHlK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data loader\n",
        "\n",
        "class LanguageModelDataLoader(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        # concatenate your articles and build into batches\n",
        "        \n",
        "        np.random.shuffle(self.dataset)\n",
        "        shuffled_data = self.dataset\n",
        "        concatenated_data = []\n",
        "        for i in shuffled_data:\n",
        "            concatenated_data.extend(i)\n",
        "        concatenated_data = np.asarray(concatenated_data)\n",
        "        new_len = concatenated_data.shape[0] - concatenated_data.shape[0]%self.batch_size\n",
        "        temp_data = concatenated_data[:new_len]\n",
        "        concatenated_data = temp_data.reshape(self.batch_size, temp_data.shape[0]//self.batch_size)\n",
        "        seq_len = 0\n",
        "        j = 0\n",
        "        while(j<concatenated_data.shape[1]):\n",
        "            tmp = np.random.randn(1)\n",
        "            if(tmp > 0.05):\n",
        "                seq_len = np.random.normal(70,5,1)\n",
        "            else:\n",
        "                seq_len = np.random.normal(35,5,1)\n",
        "            if(j+seq_len<(concatenated_data.shape[1])):\n",
        "                batch_arr = np.zeros((self.batch_size,int(seq_len)-1))\n",
        "                batch_otp = np.zeros((self.batch_size,int(seq_len)-1))\n",
        "                for i in range(concatenated_data.shape[0]-1):\n",
        "                    batch_arr[i] = concatenated_data[i,j:j+int(seq_len)-1]\n",
        "                    batch_otp[i] = concatenated_data[i,j+1:j+int(seq_len)]\n",
        "                j = j+int(seq_len)\n",
        "                yield (batch_arr, batch_otp)\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SdVDScarKHli",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "load_data = LanguageModelDataLoader(dataset,80)\n",
        "k = 0\n",
        "for i,(j,k) in enumerate(load_data):\n",
        "    print(j.shape, k.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NJsT2cnYKHl3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "        TODO: Define your model here\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, nlayers):\n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.nlayers=nlayers\n",
        "        self.embedding = nn.Embedding(vocab_size,embed_size) # Embedding layer\n",
        "        self.rnn = nn.LSTM(input_size = embed_size,hidden_size=hidden_size,num_layers=nlayers, bidirectional=True) # Recurrent network\n",
        "        self.scoring = nn.Linear(2*hidden_size,vocab_size) # Projection layer\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        batch_size = x.size(1)\n",
        "        embeddings = self.embedding(x)\n",
        "        output_lstm,hidden = self.rnn(embeddings, hidden)\n",
        "        embeddings = embeddings.detach().cpu().numpy()\n",
        "        del embeddings\n",
        "        output_lstm_flatten = output_lstm.view(-1,2*self.hidden_size)\n",
        "        output_flatten = self.scoring(output_lstm_flatten)\n",
        "        output_lstm_flatten = output_lstm_flatten.detach().cpu().numpy()\n",
        "        del output_lstm_flatten\n",
        "        torch.cuda.empty_cache()\n",
        "        return output_flatten.view(-1, batch_size, self.vocab_size), hidden\n",
        "        \n",
        "        \n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z-hoWFNUKHmP",
        "colab_type": "code",
        "outputId": "c67483ff-4f9c-4e4a-fb30-b534e8ef8cf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=len(vocab), embed_size=400, hidden_size = 1150, nlayers=3)\n",
        "model.to(device)\n",
        "for i,(j,k) in enumerate(load_data):\n",
        "    inputs = torch.LongTensor(j).to(device)\n",
        "    output = model(inputs)\n",
        "    print(output.shape)\n",
        "#     output = output.detach().cpu().numpy()\n",
        "    inputs = inputs.detach().cpu().numpy()\n",
        "    del output\n",
        "    del inputs\n",
        "    torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-144ed795ce99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "b7ul2OALKHms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# model trainer\n",
        "\n",
        "class LanguageModelTrainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        self.optimizer = Adam(model.parameters(), lr = 0.001)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "            print(epoch_loss)\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        self.optimizer.zero_grad()\n",
        "        inputs = torch.LongTensor(inputs).to(device)\n",
        "        inputs = torch.transpose(inputs,0,1).to(device)\n",
        "        targets = torch.LongTensor(targets).to(device)\n",
        "        targets = torch.transpose(targets,0,1).to(device)\n",
        "        hidden = None\n",
        "        outputs, hidden = model(inputs, hidden)\n",
        "        targets = targets.contiguous()\n",
        "        outputs = outputs.view(-1,outputs.size(2))\n",
        "        loss = self.criterion(outputs, targets.view(-1))\n",
        "        #loss = Variable(loss, requires_grad=True)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "        \n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.prediction(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generation(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generation(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.prediction(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8RpODQW7KHnH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TestLanguageModel:\n",
        "    def prediction(inp, model):\n",
        "        with torch.no_grad():\n",
        "            outputs_array = np.zeros((inp.shape[0],len(vocab)))\n",
        "            inputs = torch.LongTensor(inp).to(device)\n",
        "            inputs = torch.transpose(inputs,0,1)\n",
        "            hidden=None\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            outputs_array = outputs[-1,:,:].cpu().numpy()\n",
        "            outputs_array = outputs_array.astype(np.float64)\n",
        "        return outputs_array\n",
        "            \n",
        "    def generation(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"\n",
        "        generated_output = []\n",
        "        with torch.no_grad():\n",
        "            inputs = torch.LongTensor(inp).to(device)\n",
        "            inputs = torch.transpose(inputs,0,1)\n",
        "            hidden = None\n",
        "            outputs, hidden = model(inputs, hidden)\n",
        "            temp = outputs[-1,:,:]\n",
        "            _,current_word = torch.max(temp,dim=1)\n",
        "            generated_output.append(current_word.cpu().numpy())\n",
        "            for j in range(1,forward):\n",
        "              word = current_word\n",
        "              word = word.view(1,-1)\n",
        "              output, hidden = model(word, hidden)\n",
        "              _,current_word = torch.max(temp,dim=1)\n",
        "              generated_output.append(current_word.cpu().numpy())\n",
        "        generated_matrix = np.asarray(generated_output)\n",
        "        print(generated_matrix.shape)\n",
        "        return generated_matrix.T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1VPlk-A7KHnv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "NUM_EPOCHS = 8\n",
        "BATCH_SIZE = 80\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jBoB_JYoKHn7",
        "colab_type": "code",
        "outputId": "d561e116-a325-4d38-95a6-7c92ae645df2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving models, predictions, and generated words to ./experiments/1555388894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OjLlJCs_KHog",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LanguageModel(vocab_size=len(vocab), embed_size=400, hidden_size = 1150, nlayers=3)\n",
        "model.to(device)\n",
        "loader = LanguageModelDataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "trainer = LanguageModelTrainer(model=model, loader=loader, max_epochs=NUM_EPOCHS, run_id=run_id)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B5gMwgBzbMa6",
        "colab_type": "code",
        "outputId": "689ada3b-dbcf-4f78-8c98-2a43a10dcae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        }
      },
      "cell_type": "code",
      "source": [
        "best_nll = 1e30\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  nll = trainer.test()\n",
        "  if nll < best_nll:\n",
        "    best_nll = nll\n",
        "    print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [1/8]   Loss: 10.4125\n",
            "Saving model, predictions and generated output for epoch 0 with NLL: 10.412534264643355\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [1/8]   Loss: 10.4125\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [1/8]   Loss: 10.4125\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [1/8]   Loss: 10.4125\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [1/8]   Loss: 10.4125\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [1/8]   Loss: 10.4125\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [1/8]   Loss: 10.4125\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [1/8]   Loss: 10.4125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cMh7CDfGKHo1",
        "colab_type": "code",
        "outputId": "cef248be-7177-4b65-dc9b-999e7e711198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1669
        }
      },
      "cell_type": "code",
      "source": [
        "best_nll = 1e30 \n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "    "
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(10.4114, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(20.6574, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(29.5937, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(38.2340, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(45.7019, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(53.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(60.6593, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "[TRAIN]  Epoch [2/8]   Loss: 8.6656\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [2/8]   Loss: 7.7256\n",
            "Saving model, predictions and generated output for epoch 0 with NLL: 7.725634237300708\n",
            "tensor(7.1216, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(14.1562, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(21.3414, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(28.5622, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(35.8204, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(43.1740, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "[TRAIN]  Epoch [3/8]   Loss: 7.1957\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [3/8]   Loss: 7.7838\n",
            "tensor(6.9902, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(13.8153, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(20.6169, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(27.3100, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(33.9565, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(40.5700, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(47.1523, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(53.7970, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "[TRAIN]  Epoch [4/8]   Loss: 6.7246\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [4/8]   Loss: 7.3275\n",
            "Saving model, predictions and generated output for epoch 2 with NLL: 7.327521018722081\n",
            "tensor(6.2813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(12.4331, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(18.5158, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(24.6243, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(30.7742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "[TRAIN]  Epoch [5/8]   Loss: 6.1548\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [5/8]   Loss: 7.1841\n",
            "Saving model, predictions and generated output for epoch 3 with NLL: 7.184103101210058\n",
            "tensor(5.9151, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(11.9069, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(17.8470, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(23.8247, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(29.6729, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(35.5568, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(41.3786, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "[TRAIN]  Epoch [6/8]   Loss: 5.9112\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [6/8]   Loss: 7.1729\n",
            "Saving model, predictions and generated output for epoch 4 with NLL: 7.1729240054592065\n",
            "tensor(5.7804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(11.3574, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(16.9191, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(22.4035, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(27.9818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(33.3472, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "[TRAIN]  Epoch [7/8]   Loss: 5.5579\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [7/8]   Loss: 7.2481\n",
            "tensor(5.3848, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(10.5931, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(15.7799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(20.8949, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(25.9937, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(31.0492, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "[TRAIN]  Epoch [8/8]   Loss: 5.1749\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [8/8]   Loss: 7.3575\n",
            "tensor(4.9403, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(9.7766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(14.6876, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(19.3967, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(24.1741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(29.0193, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(33.6924, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "tensor(38.3009, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "[TRAIN]  Epoch [9/8]   Loss: 4.7876\n",
            "(10, 32)\n",
            "(10, 128)\n",
            "[VAL]  Epoch [9/8]   Loss: 7.4773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lEeaM4c4KHp4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "fdde53f3-ba46-49c3-9211-89dbd1687497"
      },
      "cell_type": "code",
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSqYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "y24qSj20KHqE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "outputId": "6e430155-8176-404a-9d5c-ae41ab86a87a"
      },
      "cell_type": "code",
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input | Output #0: while the group was en route , but only three were ultimately able to attack . None of them were | a a a a a a a a a a\n",
            "Input | Output #1: <unk> , where he remained on loan until 30 June 2010 . <eol> = = = Return to Manchester United | in in in in in in in in in in\n",
            "Input | Output #2: 25 April 2013 , denoting shipments of 500 @,@ 000 copies . <eol> The song became One Direction 's fourth | @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@\n",
            "Input | Output #3: , and Bruce R. ) one daughter ( Wendy J. <unk> ) and two grandchildren , died in <unk> , | and and and and and and and and and and\n",
            "Input | Output #4: Warrior were examples of this type . Because their armor was so heavy , they could only carry a single | . . . . . . . . . .\n",
            "Input | Output #5: the embassy at 1 : 49 and landed on Guam at 2 : 23 ; twenty minutes later , Ambassador | @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@\n",
            "Input | Output #6: <unk> , $ 96 million USD ) . Damage was heaviest in South Korea , notably where it moved ashore | a a a a a a a a a a\n",
            "Input | Output #7: The <unk> were condemned as <unk> by <unk> , who saw the riots as hampering attempts to resolve the situation | \" \" \" \" \" \" \" \" \" \"\n",
            "Input | Output #8: by a decision made by the War Office in mid @-@ 1941 , as it was considering the equipment to | a a a a a a a a a a\n",
            "Input | Output #9: Division crossed the <unk> at a number of places and climbed the hills quietly toward the 9th Infantry river line | \" \" \" \" \" \" \" \" \" \"\n",
            "Input | Output #10: = <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> On 6 November | , , , , , , , , , ,\n",
            "Input | Output #11: of the World from 9th Avenue \" . This is regarded as his most famous work . It is considered | @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@\n",
            "Input | Output #12: — <unk> @-@ 10 , <unk> @-@ 12 , <unk> @-@ 16 , <unk> @-@ 17 — were all converted | a a a a a a a a a a\n",
            "Input | Output #13: And now he has . \" <eol> = = Family = = <eol> <unk> lived 37 of his years in | in in in in in in in in in in\n",
            "Input | Output #14: Hell to which he has been condemned for <unk> . Eliot , in a letter to John <unk> dated 27 | a a a a a a a a a a\n",
            "Input | Output #15: Luoyang area , fulfilling his duties in domestic affairs . <eol> In the autumn of <unk> , he met Li | a a a a a a a a a a\n",
            "Input | Output #16: Power said they enjoyed Block Ball and its number of stages , but wondered how its eight <unk> of memory | 's 's 's 's 's 's 's 's 's 's\n",
            "Input | Output #17: by Lloyd F. Lonergan . The cameraman was Jacques <unk> . <eol> = = Release and reception = = <eol> | = = = = = = = = = =\n",
            "Input | Output #18: alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 | a a a a a a a a a a\n",
            "Input | Output #19: while attacking a ship at <unk> in the Dutch East Indies ; the loss was compounded by the fact that | a a a a a a a a a a\n",
            "Input | Output #20: first raised in 2007 by the member of parliament ( MP ) for <unk> . The gangsters may have run | @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@\n",
            "Input | Output #21: Species are also non @-@ spiny <unk> and includes both large trees with stout stems up to 30 metres ( | a a a a a a a a a a\n",
            "Input | Output #22: \" : specific design issues with the building 's energy efficiency included the fact that the largest room in the | game game game game game game game game game game\n",
            "Input | Output #23: were reported to support over 300 @,@ 000 households in the Brazilian state of <unk> in 2005 , and in | in in in in in in in in in in\n",
            "Input | Output #24: port . <unk> in Vietnam also warned for the potential of heavy rainfall due to the dissipating Tropical Depression <unk> | <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "Input | Output #25: T @-@ numbers in their tropical cyclone products . The following example is from discussion number 3 of Tropical Depression | @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@ @-@\n",
            "Input | Output #26: South Australia hosted the three @-@ game semi @-@ final series against the New South Wales <unk> . Both teams | , , , , , , , , , ,\n",
            "Input | Output #27: Perth from contention and secured the last finals spot for the <unk> . <eol> = = = Statistical leaders = | = = = = = = = = = =\n",
            "Input | Output #28: deemed it an \" amazing pop song \" , lauding the group 's falsetto and its \" head @-@ <unk> | <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "Input | Output #29: , but began patrolling the English Channel after <unk> @-@ 6 pioneered a route past British <unk> nets and mines | a a a a a a a a a a\n",
            "Input | Output #30: production executives to let him direct . He had already discussed the film with <unk> and Cohen , and felt | an an an an an an an an an an\n",
            "Input | Output #31: and Nick <unk> at Studio <unk> in Los Angeles , California , and was released on August 1 , 2006 | in in in in in in in in in in\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-ASYz14gKHqW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}